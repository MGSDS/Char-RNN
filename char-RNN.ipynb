{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e60c5ce3-d6c5-4d45-8043-240e69db609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9189d8cd-a8a5-4641-9803-55a818d8e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/macbeth.txt\"\n",
    "\n",
    "text = (open(filename).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c7f87c-9fb0-43c2-bcc4-5baf03286cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    def __init__(self, text, chunk_sz):\n",
    "        super().__init__()\n",
    "        self.__chunk_sz = chunk_sz\n",
    "        self.__chars = sorted(list(set(text.lower() + text.upper())))\n",
    "        self.__char2idx = { char : i for i, char in enumerate(self.__chars)}\n",
    "        self.__idx2char = { i : char for i, char in enumerate(self.__chars)}\n",
    "        self.__text = text\n",
    "            \n",
    "    def encode(self, char):\n",
    "        return self.__char2idx[char]\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        return self.__idx2char[idx]\n",
    "            \n",
    "    def decode_str(self, idxs):\n",
    "        return ''.join([self.decode(idx.item()) for idx in idxs])\n",
    "    \n",
    "    def encode_str(self, string):\n",
    "        return torch.LongTensor([self.encode(char) for char in string])\n",
    "    \n",
    "    def get_vocab_sz(self):\n",
    "        return len(self.__chars)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.__text) - self.__chunk_sz\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.__text[idx : idx + self.__chunk_sz]\n",
    "        return (self.encode_str(sample[:-1]),\n",
    "                self.encode_str(sample[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c424f77d-a83b-46c0-a02f-7784373f25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3315f4bf-abbc-44ea-9637-d89d0ae7f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextData(text, 50)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "badf2206-66be-4b11-a495-bcedfe5b0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6804bfde-8c89-45df-9858-a61d6a8bf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_sz, vocab_sz, emb_dim):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.__embedding = nn.Embedding(vocab_sz, emb_dim)\n",
    "        self.__lstm = nn.LSTM(emb_dim, hidden_sz, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.__linear = nn.Linear(hidden_sz, vocab_sz)\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        x = self.__embedding(x)\n",
    "        x, (h, c) = self.__lstm(x, (h, c))\n",
    "        x = x.reshape(x.shape[0] * x.shape[1], self.hidden_sz)\n",
    "        x = self.__linear(x)\n",
    "        return x, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ddacb9-6608-4fff-a82f-ca1fd3f3b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_layers=5, emb_dim=100, hidden_sz=300, vocab_sz=dataset.get_vocab_sz()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6baec322-5ea5-4e6a-9a32-c27084557531",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(params=model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87624a57-2802-4d4b-8b7a-d50a0abd9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25ec9fe0-2e61-49f1-9cfe-bd2955f43668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9b4725a3e44d5ea2dca6e8ecb57529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "erange = trange(epochs)\n",
    "loss_hist = []\n",
    "model.train()\n",
    "for i in erange:\n",
    "    running_loss = 0.0\n",
    "    runs = 0\n",
    "    for batch_X, batch_Y in dataloader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        h = torch.zeros((model.num_layers, len(batch_X), model.hidden_sz)).to(device)\n",
    "        c = torch.zeros((model.num_layers, len(batch_X), model.hidden_sz)).to(device)\n",
    "        out, (_, _) = model(batch_X, h, c)\n",
    "        loss = loss_fn(out, batch_Y.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_hist.append(loss)\n",
    "        running_loss += loss\n",
    "        runs += 1\n",
    "    erange.desc = f'loss: {running_loss/runs:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed3f55d8-92b1-44e4-925c-a137f263a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "torch.save(model.state_dict(), 'pretrained/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be26a659-8017-476e-bbfd-1f57e3baeee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (_LSTM__embedding): Embedding(68, 100)\n",
       "  (_LSTM__lstm): LSTM(100, 300, num_layers=5, batch_first=True, dropout=0.2)\n",
       "  (_LSTM__linear): Linear(in_features=300, out_features=68, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('pretrained/model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abd86bfe-248b-408d-8c8d-b783f936f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This the Sytor'd\n",
      "\n",
      "   Seyw. Euery to my blood, this Rather,\n",
      "Thou seest the Heauens, as they made their prechence.\n",
      "Cousins, a most alle,\n",
      "Which was powre in meet, and vnfolt.\n",
      "The which must consent:\n",
      "Where is the from them: hole of the Royall Donaland\n",
      "With a mast of Thrancar, as a day againe\n",
      "Goues, the pleasure of the time\n",
      "With vs in Rings. Thou soysones\n",
      "That truts then speake: the dayy aboulce, and our,\n",
      "By his the Loues, and what crayse\n",
      "Compell in the Moon'd?\n",
      "  Lenox. Make crame our thine poore Mancolme, heare it be with him, but constrained thinke\n",
      "\n",
      "   Banq. This Guest of Summer,\n",
      "As basis buried; in forth, in many and fraind,\n",
      "Then the Tyrants ha's mabord: they stare abiuile haue all:\n",
      "And be this dead\n",
      "\n",
      "   Macb. If Chance will haue me King:\n",
      "I ha's to make them\n",
      "\n",
      "   Banq. That trusted home,\n",
      "Might yet enkindle to this Doine, and (tinde,\n",
      "Then the Charme is firme and good:\n",
      "Our sping is, shalt be, before the Groomes\n",
      "Vnment, the good Donquo, hawne with you.\n",
      "\n",
      "Exeunt. Lords.\n",
      "\n",
      "Sirrha, a word with you: At\n"
     ]
    }
   ],
   "source": [
    "start_text = \"This\"\n",
    "n = 1000\n",
    "\n",
    "encoded = dataset.encode_str(start_text).unsqueeze(0).to(device)\n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    h = torch.zeros((model.num_layers, 1, model.hidden_sz)).to(device)\n",
    "    c = torch.zeros((model.num_layers, 1, model.hidden_sz)).to(device)\n",
    "    for i in range(n):\n",
    "        out, (h, c) = model(encoded, h, c)\n",
    "        out = torch.argmax(out[-1])\n",
    "        encoded = torch.LongTensor([out]).unsqueeze(1).to(device)\n",
    "        output.append(out)\n",
    "    \n",
    "print(start_text + \"\".join(dataset.decode_str(output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
